# 03. API 명세 (API Specification)

이 프로젝트는 하이브리드 AI 연동 방식을 채택하고 있어, API 명세는 로컬 추론과 클라우드 추론 두 가지로 나뉩니다.

## 1. 로컬 LLM 연동 (Client-Side Library)

현재 MVP는 백엔드 API 호출 없이 클라이언트 측 라이브러리를 직접 사용하여 AI 추론을 수행합니다.

*   **라이브러리**: `@mlc-ai/web-llm`
*   **핵심 함수**: `CreateMLCEngine(modelId, engineConfig)`
    *   `modelId`: 사용할 모델의 ID (예: `Llama-3.1-8B-Instruct-q4f32_1-MLC`)
    *   `engineConfig`: 엔진 설정 객체. 모델 로딩 진행 상태를 콜백으로 받을 수 있습니다.
*   **채팅 함수**: `engine.chat.completions.create({ messages, stream: true })`
    *   `messages`: OpenAI API와 유사한 형식의 메시지 배열 (`[{ role: 'user' | 'assistant', content: '...' }]`)
    *   `stream: true`: 응답을 스트리밍 방식으로 받습니다.
*   **특징**:
    *   별도의 API 엔드포인트가 없습니다.
    *   모든 로직은 `src/hooks/use-local-chat.ts` 내에서 처리됩니다.
    *   네트워크 요청은 모델 파일(.wasm 등)을 다운로드할 때만 발생합니다.

## 2. 클라우드 LLM 연동 (Internal API - 향후 확장)

클라우드 기능을 활성화할 경우, 다음과 같은 내부 API 엔드포인트를 사용하게 됩니다.

*   `POST /api/chat`
    *   **설명**: 사용자의 메시지를 받아 클라우드 LLM API로 전달하고 응답을 스트리밍합니다.
    *   **요청 본문**:
        ```json
        {
          "model": "gpt-4",
          "messages": [ ... ],
          "temperature": 0.7
        }
        ```
    *   **응답**: Server-Sent Events (SSE) 또는 ReadableStream을 사용한 텍스트 스트림.

*   `/api/auth/**`
    *   **설명**: Supabase Auth와 연동하여 로그인, 로그아웃, 회원가입 등 사용자 인증을 처리합니다. NextAuth.js 또는 Supabase의 `ssr` 패키지를 통해 구현됩니다.

*   `/api/settings`
    *   **설명**: 사용자의 API 키, 기본 모델 설정 등 개인화된 정보를 관리하기 위한 CRUD API입니다.

## DB 연동 (Supabase Client)

클라우드 모드에서는 Supabase 클라이언트 라이브러리(`@supabase/supabase-js`)를 사용하여 데이터베이스와 직접 통신합니다. 이는 주로 서버 측 로직(API 라우트, 서버 컴포넌트)에서 RLS(Row-Level Security) 정책에 따라 안전하게 실행됩니다.
