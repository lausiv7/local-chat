# 06. 개발 가이드 (Development Guide)

이 가이드는 Chatbot UI Accelerator 애플리케이션의 클론 및 개발 환경 설정을 안내합니다.

## 1. 레포지토리 클론 및 의존성 설치

```bash
# git clone <your-repo-url>
# cd <your-repo-name>
npm install
```

## 2. 환경변수 설정 (.env.local)

현재 프로젝트는 **로컬 LLM 추론 모드**로 실행되므로, 별도의 API 키나 Supabase 연동 없이도 즉시 실행 가능합니다.

`.env.local` 파일은 클라우드 기능을 활성화할 때 필요합니다. 해당 단계에서는 아래와 같이 Supabase 정보를 입력하게 됩니다.

```bash
# .env.local.example 파일을 복사하여 .env.local 생성

# Supabase (향후 사용)
NEXT_PUBLIC_SUPABASE_URL=YOUR_SUPABASE_URL
NEXT_PUBLIC_SUPABASE_ANON_KEY=YOUR_SUPABASE_ANON_KEY

# Cloud LLM API Keys (향후 사용)
OPENAI_API_KEY=YOUR_OPENAI_API_KEY
```

## 3. 애플리케이션 실행

아래 명령어를 사용하여 개발 서버를 시작합니다.

```bash
npm run dev
```
기본적으로 `http://localhost:9002`에서 앱을 확인할 수 있습니다.

## 4. 로컬 LLM 모델 연동

AI 추론에 대한 모든 설정은 `src/hooks/use-local-chat.ts` 파일에서 관리됩니다.

```typescript
// src/hooks/use-local-chat.ts

// 사용할 모델 ID를 변경하여 다른 모델 테스트 가능
const MODEL_ID = 'Llama-3.1-8B-Instruct-q4f32_1-MLC';

// ...

const initializeEngine = async () => {
    // ...
    engine.current = await CreateMLCEngine(MODEL_ID, { 
        initProgressCallback: (progress) => {
            setModelStatus(progress.text);
        }
    });
    // ...
};
```
*   **모델 변경**: `MODEL_ID` 상수를 변경하여 `@mlc-ai/web-llm`에서 지원하는 다른 모델을 테스트할 수 있습니다.
*   **참고**: 모델 목록은 [WebLLM 공식 문서](https://github.com/mlc-ai/web-llm)에서 확인할 수 있습니다.

## 5. PWA 및 캐싱

이 프로젝트는 서비스 워커(`public/sw.js`)를 사용하여 PWA 캐싱을 구현합니다. 개발 모드(`npm run dev`)에서는 서비스 워커가 활성화되지 않을 수 있습니다. 프로덕션 빌드 후(`npm run build && npm start`) 정상적으로 동작을 확인할 수 있습니다.

## 6. 확장 및 커스터마이징

*   **UI 컴포넌트**: `src/components/ui` (ShadCN) 와 `src/components/chat` (채팅 관련) 폴더에서 UI를 수정하거나 추가할 수 있습니다.
*   **채팅 로직**: `src/hooks/use-local-chat.ts` 파일을 수정하여 대화 흐름, 상태 관리 등을 변경할 수 있습니다.
*   **사이드바 및 레이아웃**: `src/components/app-sidebar.tsx`와 `src/app/layout.tsx`에서 전체적인 앱 레이아웃을 커스터마이징합니다.
